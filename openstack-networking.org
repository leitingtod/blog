#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="worg.css" />
#+HTML_HEAD_EXTRA: <link rel="alternate stylesheet" type="text/css" href="worg.css" />
#+OPTIONS: email:nil <:nil \n:nil ^:nil p:nil pri:nil prop:nil todo:nil
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_FILE_NAME: /home/liu/program/work/pkg-openstack/doc/RabbitMQ Learning.html
#+Author: liuyazhou
#+EMAIL: liuyazhou@fronware.com
#+TITLE: OpenStack Networking Guide
#+SUBTITLE: v1.0.0
* Introduction to networking[fn:1]
** basic networking
*** Ethernet
Ethernet is a networking protocol, specified by the IEEE 802.3 standard. Most
wired network interface cards (NICs) communicate using Ethernet.

In the OSI model of networking protocols, *Ethernet* occupies the second layer,
which is known as the *data link layer*. When discussing *Ethernet*, you will
often hear terms such as *local network*, *layer 2*, *L2*, *link layer* and
*data link layer*.

| layer | OSI                | TCP/IP            | PROTOCOAL                                           |
|-------+--------------------+-------------------+-----------------------------------------------------|
|     / | <                  | <                 | <                                                   |
|     1 | physical layer     |                   |                                                     |
|     2 | data link layer    | link layer        | ATM, DSL, ISDN, Ethernet, etc.                      |
|-------+--------------------+-------------------+-----------------------------------------------------|
|     3 | network layer      | internet layer    | ICMP, IGMP, BGP, OSPF, RIP                          |
|-------+--------------------+-------------------+-----------------------------------------------------|
|     4 | transport layer    | transport layer   | TCP, UDP                                            |
|-------+--------------------+-------------------+-----------------------------------------------------|
|     5 | session layer      |                   | (TCP)FTP, HTTP/HTTPS, SSH, TELNET, POP3, SMTP, IMAP |
|     6 | presentation layer | application layer | (UDP)BOOTP, NTP                                     |
|     7 | applicaation layer |                   | (TCP&UDP)DNS, ECHO; (OTHERS) DHCP,ARP,SNMP          |

In network diagrams (including those generated by the OpenStack dashboard) an
Ethernet network is often depicted as if it was a single bus. You’ll sometimes
hear an Ethernet network referred to as a *layer 2 segment*.

In particular, in an OpenStack environment, every virtual machine instance has a
unique MAC address, which is different from the MAC address of the compute host.

A MAC address has 48 bits and is typically represented as a hexadecimal string,
such as 08:00:27:b9:88:74. The MAC address is hard-coded into the NIC by the
manufacturer, although modern NICs allow you to change the MAC address
programmatically.

通常使用同一物理层的设备之间必然通过相同的传输介质直接相互连接，（如交叉双绞线直
接连接的两台主机）；但是两组其传输介质并非直接相连的网络设备，如果它们的传输介质
通过工作在物理层的扩展设备如中继器和集线器等转接连接，则仍然被视为同一物理层中的
设备，是一个而非两个网段。另外，工作在*数据链路层*或更高层的设备如网桥、交换机、
路由器等等，由它们连接起来的两组设备仍然分别处于各自独立的物理层，因此是两个网段。

在以太网环境中，一个网段其实也就是一个冲突域（碰撞域）。同一网段中的设备共享（包
括通过集线器等设备中转连接）同一物理总线，在这一总线上执行CSMA/CD（载波监听多路
访问/冲突检测）机制。不同网段间不共享同一物理层，因此不会跨网段发生冲突（碰撞）。
现代高速以太网通常使用交换机代替集线器，交换机是工作在数据链路层的设备，由它转接
的两组设备不在同一网段中。事实上，交换机为连在其上的每一个独立设备各自划分出一个
独立的网段，每个网段只包含两个设备——交换机本身，和这个独立设备。这样，交换机就能
隔离冲突，提高网络的利用率和总体性能。

broadcast domain: every host can send frames directly to every other host,
broadcast is also supported in a broadcast domain.

NIC: promiscuous mode - nic will pass all ethernet frames to the OS, even if the
MAC doesn't match.

forwarding table|forwarding information base: the mappings of MAC address to
switch port in a table used by a switch.
*** VLANs
VLAN is a networking technology that enables a single switch to act as if it was
multiple independent switches. Specifically, two hosts that are connected to the
same switch but on different VLANs do not see each other's traffic.

OpenStack is able to take advantage of VLANs to *isolate the traffic of
/different tenants/*, even if the tenants happen to have instances running on
the same computer host.
*** Subnets and ARP
IP addresses are broken up into two parts: a network number and a host
identifier. Two hosts are on *the same subnet* if they *have the same network*
*number*. Recall that two hosts can *only* communicate directly over Ethernet if
they are on the same *local network*. ARP assumes that all machines that are in
the same subnet are on the same local network.

Creating CIDR subnets including a multicast address or a loopback address cannot
be used in an OpenStack environment. For example, creating a subnet using
224.0.0.0/16 or 127.0.1.0/24 is not supported.
*** TCP/UDP/ICMP
** network components
*** Switches
Switches are *Multi-Input Multi-Output* devices that enable packets to travel
from one node to another. Switches connect *hosts* that *belong* to the *same
layer-2* network. Switches enable forwarding of the packet received on one port
(input) to another port (output) so that they reach the desired destination
node. Switches operate at layer-2 in the networking model. They *forward* the
traffic based on *the destination Ethernet address* in the packet header.
*** Routers
Routers are special devices that enable packets to travel from one *layer-3*
network to another. Routers enable *communication* between two nodes on
*different layer-3* networks that are not directly connected to each
other. Routers operate at layer-3 in the networking model. They *route* the
traffic based on *the destination IP address* in the packet header.

*** Firewalls
Firewalls are used to regulate traffic to and from a host or a network. A
firewall can be:
- either a specialized device connecting two networks
- or a software-based filtering mechanism implemented on an operating system.
Firewalls are used to *restrict traffic* to a host based on *the rules defined
on* the host. They can filter packets based on *several criteria* such as
*source* *IP address*, *destination IP address*, *port numbers*, *connection
state*, and so on. It is primarily used to protect the hosts from *unauthorized*
access and *malicious* attacks. Linux-based operating systems implement
firewalls through iptables.

*** Load balancers
Load balancers can be *software-based* or *hardware-based* devices that allow
traffic to evenly be distributed across several servers.

By *distributing* the traffic across *multiple servers*, it *avoids overload* of
a single server thereby preventing a *single point of failure* in the product.
** tunnel technologies
Tunneling allows one network protocol to *encapsulate another payload protocol*
such that packets from the payload protocol are passed as data on the delivery
protocol. For example, this can be used to pass data securely over an untrusted
network.
*** GRE
GRE carries IP packets with private IP addresses over the Internet using
delivery packets with public IP addresses.
*** VxLAN[fn:4]
:PROPERTIES:
:EXPORT_TITLE: VxLAN Learning
:EXPORT_FILE_NAME: /home/liu/program/work/pkg-openstack/doc/VxLAN Learning.html
:END:
在云计算数据中心部署中，各客户的应用程序之间需要逻辑隔离。而现有VLAN分段技术难以
满足多租户和规模扩展。原有VLAN技术有如下瓶颈：
- VLAN.不能提供足够多（超过4096个）的分段。
- VLAN网络绑定到物理网络结构，限制分布式云数据中心要求虚拟机的移动性或灵活性。
- 大量VLAN配置接口对汇聚层STP(Spanning Treep Protocol，生成树协议)带来高负荷。

**** VXLAN方案基本原理
VXLAN(Virtual eXtensible Local Area Network)是一种将二层报文用三层协议进行封装的
技术，可以对二层网络在三层范围进行扩展。它应用于数据中心内部，使虚拟机可以在互相
连通的三层网络范围内迁移，而不需要改变IP地址和MAC地址，保证业务的连续性。VXLAN采
用24bit的网络标识，使用户可以创建16M相互隔离的虚拟网络，突破了目前广泛采用的VLAN
所能表示的4K个隔离网络的限制，这使得大规模多租户的云环境中具有了充足的虚拟网络分
区资源。
#+CAPTION: VxLAN Frame
[[./images/VxLAN-Frame.png]]

VXLAN通过在物理网络的边缘设置智能实体VTEP(VXLAN Tunnel End Point)，实现了虚拟网
络和物理网络的隔离。VTEP之间建立隧道，在物理网络上传输虚拟网络的数据帧，物理网络
不感知虚拟网络。（VTEP相关当虚拟的PPoPE)

**** VXLAN的客户收益
- 虚拟机可以跨三层网络实时迁移. ，不需要对物理网络重新配置，业务不中断；
- 废弃STP协议，充分利用链路；
- 可以创建16M互相隔离的虚拟子网，充分满足多租户数据中心的需求；
- 接入交换机只学习物理服务器的MAC地址，不需要学习每个虚拟机的MAC，极大地节省MAC表空间提升的交换性能。

**** VXLAN的组播问题
VXLAN的数据平面需要依赖物理交换机的组播功能(IGMP、PIM)，将VXLAN内的广播映射为组
播，而物理交换机对于IGMP组播组的数量支持往往有限，它虽然能够利用将多个VXLAN加入
同一个组播组的方法缓解交换机组播组规格不足的问题，但存在网络性能下降等问题。另外
广域网络通常不支持组播转发，无法直接实现VXLAN在不同数据中心之间的扩展，而需要开
发新的机制将组播映射成单播发送至其他数据中心。

组播的问题可通过SDN controller与VXLAN配合来解决。解决主播的问题主要技术要点如下：
- SDN Controller兼做ARP代理。SDN Controller兼做ARP 代理(类似Router)，获知(MAC、
  IP)对，在不同DC SDN Controller间交换(MAC、IP)表。
- 组播抑制。VM将内层VM的MAC到外层IP(网关IP)的对应关系及时发布给SDNController，在
  SDN Controller间及时交换信息。
- 组播头端复制。其可通过头端复制，将应用层带来的组播变成多个单播。

**** VxLAN是什么
1. By far the most popular *virtualization technique in the data center* is
   VXLAN.  This has as much to do with Cisco and VMware backing the technology
   as the tech itself. That being said *VXLAN is targeted* specifically at the
   *data center* and is one of many *similar solutions* such as: *NVGRE* and
   *STT*.)  VXLAN’s *goal* is allowing *dynamic large scale* */isolated virtual
   L2 networks/* to be created for *virtualized and /multi-tenant/
   environments*.  It does this by encapsulating frames in VXLAN packets.  The
   standard for VXLAN is under the scope of the IETF NVO3 working group.

2. A VXLAN, virtual extensible local area network, allows the creation of a
   logical network for virtual machines across various networks. VXLAN
   encapsulates layer-2 Ethernet frames over layer-4 UDP packets.

**** the basic theory of operations and functionality of VXLAN[fn:5]
The VXLAN encapsulation method is IP based and provides for a virtual L2
network.  With VXLAN the full Ethernet Frame (with the exception of the Frame
Check Sequence: FCS) is carried as the payload of a UDP packet.  VXLAN utilizes
a 24-bit VXLAN header, shown in the diagram, to identify virtual networks.  This
header provides for up to 16 million virtual L2 networks.

Frame encapsulation is done by an entity known as a VXLAN Tunnel Endpoint
(VTEP.)  A VTEP has two logical interfaces: an uplink and a downlink.  The
uplink is responsible for receiving VXLAN frames and acts as a tunnel endpoint
with an IP address used for routing VXLAN encapsulated frames.  These IP
addresses are infrastructure addresses and are separate from the tenant IP
addressing for the nodes using the VXLAN fabric.  VTEP functionality can be
implemented in software such as a virtual switch or in the form a physical
switch.

VXLAN frames are sent to the IP address assigned to the destination VTEP; this
IP is placed in the Outer IP DA.  The IP of the VTEP sending the frame resides
in the Outer IP SA.  Packets received on the uplink are mapped from the VXLAN ID
to a VLAN and the Ethernet frame payload is sent as an 802.1Q Ethernet frame on
the downlink.  During this process the inner MAC SA and VXLAN ID is learned in a
local table.  Packets received on the downlink are mapped to a VXLAN ID using
the VLAN of the frame.  A lookup is then performed within the VTEP L2 table
using the VXLAN ID and destination MAC; this lookup provides the IP address of
the destination VTEP.  The frame is then encapsulated and sent out the uplink
interface.
#+CAPTION: VxLAN VTEP
[[./images/VxLAN-VTEP.png]]

Using the diagram above for reference a frame entering the downlink on VLAN 100
with a destination MAC of 11:11:11:11:11:11 will be encapsulated in a VXLAN
packet with an outer destination address of 10.1.1.1.  The outer source address
will be the IP of this VTEP (not shown) and the VXLAN ID will be 1001.

In a traditional L2 switch a behavior known as flood and learn is used for
unknown destinations (i.e. a MAC not stored in the MAC table.  This means that
if there is a miss when looking up the MAC the frame is flooded out all ports
except the one on which it was received.  When a response is sent the MAC is
then learned and written to the table.  The next frame for the same MAC will not
incur a miss because the table will reflect the port it exists on.  VXLAN
preserves this behavior over an IP network using IP multicast groups.

Each VXLAN ID has an assigned IP multicast group to use for traffic flooding
(the same multicast group can be shared across VXLAN IDs.)  When a frame is
received on the downlink bound for an unknown destination it is encapsulated
using the IP of the assigned multicast group as the Outer DA; it’s then sent out
the uplink.  Any VTEP with nodes on that VXLAN ID will have joined the multicast
group and therefore receive the frame.  This maintains the traditional Ethernet
flood and learn behavior.

VTEPs are designed to be implemented as a logical device on an L2 switch.  The
L2 switch connects to the VTEP via a logical 802.1Q VLAN trunk.  This trunk
contains an VXLAN infrastructure VLAN in addition to the production VLANs.  The
infrastructure VLAN is used to carry VXLAN encapsulated traffic to the VXLAN
fabric.  The only member interfaces of this VLAN will be VTEP’s logical
connection to the bridge itself and the uplink to the VXLAN fabric.  This
interface is the ‘uplink’ described above, while the logical 802.1Q trunk is the
downlink.
#+CAPTION: VxLAN VTEP
[[./images/VxLAN-VTEP-Switch.png]]

VXLAN is a network overlay technology design for data center networks.  It
provides massively increased scalability over VLAN IDs alone while allowing for
L2 adjacency over L3 networks.  The VXLAN VTEP can be implemented in both
virtual and physical switches allowing the virtual network to map to physical
resources and network services.  VXLAN currently has both wide support and
hardware adoption in switching ASICS and hardware NICs, as well as
virtualization software.

**** how VXLAN operates on the network.
Let’s start with the basic concept that VXLAN is an encapsulation technique.
Basically the Ethernet frame sent by a VXLAN connected device is encapsulated in
an IP/UDP packet.  The most important thing here is that it can be carried by
any IP capable device.  The only time added intelligence is required in a device
is at the network bridges known as VXLAN Tunnel End-Points (VTEP) which perform
the encapsulation/de-encapsulation.  This is not to say that benefit can’t be
gained by adding VXLAN functionality elsewhere, just that it’s not required.
#+CAPTION: VxLAN Frame
[[./images/VxLAN-Frame2.png]]
***** Providing Ethernet Functionality on IP Networks:
The source and destination IP addresses used for VXLAN are the Source VTEP and
destination VTEP.  This means that the VTEP must know the destination VTEP in
order to encapsulate the frame.  One method for this would be a centralized
controller/database.  That being said VXLAN is implemented in a decentralized
fashion, not requiring a controller.  There are advantages and drawbacks to
this.  While utilizing a centralized controller would provide methods for
address learning and sharing, it would also potentially increase latency,
require large software driven mapping tables and add network management points.
We will dig deeper into the current decentralized VXLAN deployment model.

VXLAN maintains backward compatibility with traditional Ethernet and therefore
must maintain some key Ethernet capabilities.  One of these is flooding
(broadcast) and ‘Flood and Learn behavior.’ I cover some of this behavior here
(http://www.definethecloud.net/data-center-101-local-area-network-switching) but
the summary is that when a switch receives a frame for an unknown destination
(MAC not in its table) it will flood the frame to all ports except the one on
which it was received.  Eventually the frame will get to the intended device and
a reply will be sent by the device which will allow the switch to learn of the
MACs location.  When switches see source MACs that are not in their table they
will ‘learn’ or add them.

VXLAN is encapsulating over IP and IP networks are typically designed for
unicast traffic (one-to-one.)  This means there is no inherent flood capability.
In order to mimic flood and learn on an IP network VXLAN uses IP multi-cast.  IP
multi-cast provides a method for distributing a packet to a group.  This IP
multi-cast use can be a contentious point within VXLAN discussions because most
networks aren’t designed for IP multi-cast, IP multi-cast support can be
limited, and multi-cast itself can be complex dependent on implementation.

Within VXLAN each VXLAN segment ID will be subscribed to a multi-cast group.
Multiple VXLAN segments can subscribe to the same ID, this minimizes
configuration but increases unneeded network traffic.  When a device attaches to
a VXLAN on a VTEP that was not previously in use, the VXLAN will join the IP
multi-cast group assigned to that segment and start receiving messages.

#+CAPTION: Know MAC Example
[[./images/VxLAN-Example.png]]

In the diagram above we see the normal operation in which the destination MAC is
known and the frame is encapsulated in IP using the source and destination VTEP
address.  The frame is encapsulated by the source VTEP, de-encapsulated at the
destination VTEP and forwarded based on bridging rules from that point.  In this
operation only the destination VTEP will receive the frame (with the exception
of any devices in the physical path, such as the core IP switch in this
example.)
#+CAPTION: Unknow MAC Example
[[./images/VxLAN-Example2.png]]

In the example above we see an unknown MAC address (the MAC to VTEP mapping does
not exist in the table.)  In this case the source VTEP encapsulates the original
frame in an IP multi-cast packet with the destination IP of the associated
multicast group.  This frame will be delivered to all VTEPs participating in the
group.  VTEPs participating in the group will ideally only be VTEPs with
connected devices attached to that VXLAN segment.  Because multiple VXLAN
segments can use the same IP multicast group this is not always the case.  The
VTEP with the connected device will de-encapsulate and forward normally, adding
the mapping from the source VTEP if required.  Any other VTEP that receives the
packet can then learn the source VTEP/MAC mapping if required and discard
it. This process will be the same for other traditionally flooded frames such as
ARP, etc.  The diagram below shows the logical topologies for both traffic types
discussed.

#+CAPTION: Broadcast, Multicast, Direct Unicast
[[./images/VxLAN-Example3.png]]

As discussed in Part 1 VTEP functionality can be placed in a traditional
Ethernet bridge.  This is done by placing a logical VTEP construct within the
bridge hardware/software.  With this in place VXLANs can bridge between virtual
and physical devices.  This is necessary for physical server connectivity, as
well as to add network services provided by physical appliances.  Putting it all
together the diagram below shows physical servers communicating with virtual
servers in a VXLAN environment.  The blue links are traditional IP links and the
switch shown at the bottom is a standard L3 switch or router.  All traffic on
these links is encapsulated as IP/UDP and broken out by the VTEPs.
#+CAPTION: Broadcast, Multicast, Direct Unicast
[[./images/VxLAN-Operation.png]]

VXLAN provides backward compatibility with traditional VLANs by mimicking
broadcast and multicast behavior through IP multicast groups.  This
functionality provides for decentralized learning by the VTEPs and negates the
need for a VXLAN controller.

**** Openstack Neutron using VXLAN[fn:6]						   :noexport:

在后网络2.0时代，数据通信多次加速，超过百分之五十的数据通信属于实时视频数据流，
因此，设计一个新的网络迫在眉睫。网络的发展需要跟上数据通信加速的步伐（从电路/数
据包交换到基于100G网速的复杂网络协议），网络发展的下一阶段即是软件定义网络（SDN）。

在过去的几年里，就网络发展如何跟上服务器虚拟化的演变节奏，已经有许多相关研究。从
很多企业开始投入大量资金研发SDN起，互联网的发展趋势就已经逐渐显现。关于SDN的精确
定义仍在完善之中，但是SDN总的原则和基本协议已经明确了。

作为SDN的学习辅导书，本书主要讨论了SDN最具发展前景的一种协议：OpenFlow。同时本书
也涉及到SDN的其他实现：虚拟可扩展局域网（VxLAN）。本书的编写基于OpenFlow1.0.0规
范和VxLAN草案。

**** 理论与实践:VxLAN在云数据中心组网的应用[fn:8]
***** 引言
据IDC（Intemational Data Corporation，国际数据公司）分析，社交化、移动化、大数据、
云计算等成为第三代ICT的关键词。电信运营商可提供ICT服务的主要基地——数据中心需要接
应挑战，适应新时期业务需求的变化。

在新时期，数据中心的业务需求从面向传统的机架出租、带宽出租以及简单的增值服务等应
用越来越多地向与云计算、大数据和移动化相关的需求转型，包括以下4个方面。

- 终端多、流量大：海量虚拟机接入及其带来的数据中心内、数据中心之间大量的东西向流量。
- 应用可靠性高、体验好：实现虚拟机和物理机的无缝迁移。
- 业务发放敏捷性高、可管可控能力强：按需快速实现计算、存储和网络虚拟化资源的快速
  部署以及端到端可视化运营管理。
- 混合云业务为趋势：随着公有云接受度的逐渐提高，企业IT运营成本不断降低，混合云成
  为越来越多的企业选择。

但目前数据中心网络架构仍属于传统组网，将不能很好地适应新时期的业务发展，主要体现
在以下6个方面：
- 数据中心网络架构不够扁平：仍是传统的三层网络架构，主要用于支撑传统互联网访问的
  南北向流量，而不适应云计算引入后日益增多的虚拟机访问或迁移带来的东西向流量的快
  速调整。

- 网络系统容量有限：面向越来越多的海量虚拟机部署，传统数据中心网络设备的二层MAC
  地址表项容量日显不足；云引人大量租户，将带来大量IP地址重叠，甚至MAC地址重叠。

- 二层逻辑通道有限：现有数据中心一般采用VLAN作为用户的二层逻辑通道隔离标识，难以
  满足今后大量租户多终端多业务的需求。另一方面，由于QinQ无法实现跨广域网的物理网
  络和虚拟网络端到端透传与统一标识，所以传统网络中采用QinQ进行二层逻辑通道扩展的
  情况并不适用于云数据中心。

- 数据中心网络柔性不足：现有网络设备主要由专用硬件构成，网络中各个网元与业务耦合
  紧密，整体呈刚性，不利于业务提供的灵活性和新业务开发的敏捷性。

- 面向云的端到端网络运维效率不高：在云计算引入数据中心的早期，传统数据中心的IP网
  络／以太网络尚未来得及与计算虚拟化网络很好地对接，无法实现基于租户逻辑通道颗粒
  度的端到端网络流量、流向可视化。

- 跨机房二层网络互联成本较高：为了支持跨地域虚拟机实时迁移，需要实现跨地域大二层
  组网。而基于裸光纤互联、二层专线互联等传统二层组网投入成本高。

VxLAN (virtual extensible local area network)作为业界当前主流的overlay（叠加网）
技术，具有spine（脊）-leaf(叶)扁平组网、二层逻辑通道数量大、实现大二层互联对现网
改造影响小、主流厂商设备支持较广泛、支持厂商涉及面广（从网络设备厂商到虚拟化平台
厂商）等优点，越来越多的互联网服务商及电信运营商开始关注VxLAN技术并在其数据中心
中引入，以期解决上述数据中心向云化演进过程中所面临的问题。另一方面，SDN
(software definednetworking，软件定义网络)作为网络演进的主流方向之一，且具有转发
平面与控制平面分离从而实现低成本网络部署、灵活便捷的业务提供等优势，也是广大互联
网服务商和电信运营商日益关注的技术焦点。但城域网组网复杂、设备种类繁多，因此，相
对简单、封闭且需要适应云时期新需求的数据中心是SDN最佳引入场点。综合SDN和VxLAN的
优势，很多厂商将SDN和VxLAN技术进行结合，以实现数据中心更加灵活、可靠、扩展的组网
和业务。
***** VxLAN技术实现
目前VxLAN主要有2种实现方式，其区别本质在于控制平面机制不同。
****** 无控制平面而基于多播的VxLAN
基于多播的VxLAN是IETF RFC7348定义的没有控制平面的VxLAN方案，即基于数据驱动的泛洪
再学习的方式，利用多播来传送VxLAN中的BUM (broadcast，unknown unlcast，multicast，
广播分组、未知单播分组、多播分组）流量。当网络中任意主机发起ARP请求时，网络中所
有VTEP（VxLAN tunneling end point，VxLAN隧道终端）都会收到该请求。因此，带来较大
的泛洪流量，消耗网络带宽资源，也带来安全隐患。

通过引入SDN控制器作为VxLAN的ARP proxy（代理），避免VxLAN在通信开始必须全网开
启多播以实现MAC地址学习的需求，从而降低对undelay（底层承载网）IP网的要求（不需要
全网开启多播或多播VPN），并避免了IP网络中因此而产生的大量多播流量。

****** 基于MP-BGP EVPN的VxLAN
IETF RFC7432等相关标准和草案定义了基于MP-BGP EVPN (multiprotocol border gateway
protocolethernet virtual private network，多协议边界网关协议一以太虚拟专网）控制
平面。其中，MP-BGP是EVPN的路由协议，通过VRF（virtual routing forwarding，VPN路由
转发表，以RD、RT为唯一标识）构建多租户环境，利用标准新定义的地址组EVPN发布EVPN路
由（EVPN路由以“MAC地址+IP地址”标识）。

基于MP-BGP EVPN的VxLAN方案中，有控制平面和数据平面两个平面。其中，控制平面实现
peer发现及路由学习（包括本地学习和远端学习）和分发；数据平面实现overlay的L2/L3单
播流转发及BUM流量的转发。

具体实现而言，基于MP-BGP EVPN的VxLAN技术实现主要有2种，分别如图2和图3所示。区别
在于控制平面是否VxLAN设备自身支持。

与基于多播实现的VxLAN方案相比，基于MP-BGP EVPN控制平面的VxLAN实现方案具有以下优
点：
- 由于MP-BGP EVPN利用了L3 VPN的VRF特性来传递VxLAN，因此天生支持多租户组网；

- 由于利用MP-BGP EVPN发布主机的MAC地址和IP地址，因此是一种协议驱动的地址学习，从
  而支持很高的组网扩展性；

- 当主机移动时，主机新接入的VTEP将通过MP-BGP马上更新路由表，通知VNI
  (VxLANnetwork identifier．VxLAN逻辑通道标识)内所有其他VTEP这个主机的新位置，从
  而实现了网络失败或主机移动时的网络快速收敛；

- 通过MP-BGP认证实现了VTEP peer的认证，从而提升了VTEP的安全性，可有效防范VTEP伪
  冒等网络安全隐患。

****** VxLAN技术实现现状
目前主流设备厂商的VxLAN解决方案百家争鸣。

按照控制平面实现方式划分：一种是无控制平面，但可以通过SDN控制器作为ARP代理来减少
泛洪影响，从而实现组网优化，如华三、华为VxLAN解决方案；另一种是基于MP-BGP EVPN的
控制平面来消除泛洪，如思科、阿朗MP-BGP EVPN的VxLAN解决方案。

按照组网连接拓扑划分，主要有spine（脊）-leaf(叶)架构和Mesh（网状）架构。由于
spine-leaf架构可以实现任意两点之间的一跳可达，具有高可靠、高扩展、高度扁平组网的
特性，越来越多厂商的VxLAN解决方案已开始支持spine-leaf架构。

按照网络功能实现载体分，主要有3种方式：一是解决方案全部基于纯硬件网络设备实现，
如Arista；二是解决方案全部基于纯软件实现，如VMware (NSX)；三是解决方案包括硬件网
络设备和软件vSwitch实现，包括思科NEXUS系列（包括专用虚拟交换机）、华三S系列交换
机和专用虚拟交换机、阿朗的VTEP交换机和VSR虚拟机交换机等。
***** VxLAN在云数据中心的组网应用
VxLAN在云数据中心组网应用主要有3个场景：
- 一是作为企业用户本地CPN内应用及系统与云数据中心部署的ICT系统通过大二层互联，提
  供虚拟机实时迁移、数据灾备等服务：
- 二是数据中心内部实现网络虚拟化与计算虚拟化的进一步无缝对接（计算虚拟化平台也开
  始支持VxLAN技术）；
- 三是分别部署在多个云数据中心，以现有城域网作为underlay底层架构，通过基于VxLAN
  构建的overlay网络提供虚拟机实时迁移与数据灾备的大二层通道。
****** VxLAN在云数据中心组网应用的技术要求
由于云数据中心将面临海量虚拟机和物理机接人、大量虚拟机实时迁移以及虚拟机访问的承
载需求，因此，云数据中心对VxLAhr交换机（支持VTEP建立／终结功能的交换机）的要求，
除了需要传统交换机的功能外（如安全功能、IPv6功能等），还需要与云／虚拟化相关的一
些个性化的技术要求。

1) 分布式路由：大量的东西向流量要求物理VxLAN交换机和虚拟VxLAN交换机都可以支持分
   布式路由，实现流量本地转发，且流表的稳定性与SDN控制器弱相关，甚至解耦。

2) VxLAN Mdge（桥接）和VxLAN router（路由）满足跨子网、跨二层域的访问需求。

3) 系统高扩展性：
   - 一是支持L2和L3的网络虚拟化，包括MAC地址和IP地址重用(overlapping)，为今后全
	 网虚拟化奠定网络基础，也要求物理VxLAN交换机和虚拟VxLAN交换机都可以支持地址
	 重用，从而可以灵活按需接人物理服务器和虚拟机；

   - 二是需要支持大量的二层逻辑链路标识，且支持新引入的VxLAN标识VNI与传统二层逻
	 辑通道标识VLAN的相互映射，便于与接入光网互通：

   - 三是MAC地址和lP地址转发表项足够大，且系统支持按节点线性扩展相关能力。

4) 系统高可靠性

   - 一是要求SDN控制器失效时不影响已下发建立的流表稳定运行；
   - 二是支持ECMP（等价多路径），尽可能实现网络带宽的高效利用以及网络链路的高度冗余。

5) 系统开放性和可演进性：参照业界主流标准，随着设备能力完善，逐步支持与第三方NFV互通、与第三方VxLAN交换机互通。

6) 端到端可视化：需要物理网络和虚拟网络（虚拟交换机）的端到端可管理、可配置。

7) 分权分域管理：支持面向运营商、租户以及租户内不同级别用户的多维度网络和业务的
   差异化、个性化管理。
****** 基于VxLAN的云数据中心组网解决方案
VxLAN在云数据中心组网主要有如下3种应用场景。
1) 基于VxLAN网关的混合云组网用户CPN部署的VxLAN网关通过城域网underlay网络与
   DC（data center，数据中心）中的VxLAN交换机互联，由于基于VxLAN构建了跨用户驻地
   网和电信运营商DC的大二层网络，因此可以提供用户驻地网中重要应用在电信运营商DC
   内的灾备以及虚拟机实时迁移。不仅虚拟机应用可以互通，同时，对用户驻地网与云数
   据中心之间现有IP网络也几乎无影响。另一方面，对于总部与多个分支机构互联组网模
   式，由于企业内网一般采用私网地址组网，各分支机构的IP地址可能重叠，随着今后虚
   拟机的大量引入，MAC地址也可能重叠，因此，在用户总部所在DC内采用VxLAN组网，将
   可以允许IP/MAC地址重用而不会影响用户分支已组建好的网络。用户驻地网(CPN)侧的
   VxLAN网关可以是硬件形态，也可以是安装在虚拟机上的软件形态。

2) DC内VxLAN组

   DC内采用VxLAN组网比传统的VLAN组网有更多优势：其一可以支持大量虚拟机、服务器的
   MAC地址和IP地址重叠，并解决二层逻辑通道4 096个VLAN标识限制问题，实现DC网络高
   扩展性；其二，VxLAN先天支持ECMP(equal cost multipath，等价多路径)，可以解决传
   统LAN中的生成树环路问题，并实现基于流的精细化颗粒度负载均担，充分利用网络资源
   并提供网络高冗余性；其三，DC采用VxLAN组网，可以与已支持VxLAN的计算虚拟化层无
   缝融合组网，实现虚拟网络、物理网络端到端逻辑路径可视化。
3) 跨城域网的DC间互联组网

   基于VxLAN实现跨DC互联组网，不仅可以满足新时期业务需求，而且可以降低运营成本。
   如图6所示，由于VxLAN是一种MAC in UDP的overlay网络技术，因此，可以在对跨现有DC
   间IP网络几乎无影响的情况下构建跨地域大二层网络，满足大量虚拟机实时迁移导致的
   东西向流量对广域网低时延等要求。同时，相对于传统基于裸光纤互联、构建新型OTN实
   现大二层组网具有较高的经济性；而相对于传统基于lP网络的MPLS L2 VPN二层专线网络，
   不需要全网开启MPLS，且运维管理相对简单。
***** 结束语
VxLAN技术逐步成熟，知名互联网服务商和主流电信运营商、大型企业已逐步试验或商用部
署。但是，受限于标准化进展以及厂商不同的实现能力．VxLAN技术在云数据中心大规模、
多场景商用尚有一些技术问题需要进一步深入研究，并推动解决。

- 一是VxLAN协议开销对现网的影响。VxLAN网关建立VxLAN隧道时，由于VxLAN封装会将原有
  IP分组增加50 byte开销，因此，需保证VxLAN隧道沿途所有underlayIP网络设备的MTU值
  设置得大于VxLAN报文。或者，可以在VxLAN VTEP之间先开启GRE隧道，然后再进行VxLAN
  隧道封装，此时只要GRE隧道两个端点将MTU值适当调大即可。但GRE隧道承载VxLAN隧道也
  存在一定问题，因为GRE隧道是点对点连接，每一个VxIAN隧道都需为之配置一条点对点
  GRE隧道，将带来一定的运维复杂度，

- 二是SDN控制器与VxLAN交换机之间南向接口（协议）的标准性和开放性有待提高。目前，
  SDN控制器与交换机之间南向接口协议主要有OpenFlow和OpFlex两种．OpenFlow为
  ONF(Open Networking Foundation，开放网络基金会)所定义，而OpFlex为IETF定义。但
  是，由于各厂商仍有私有实现机制在标准定义范畴之外，因此目前商用解决方案中异厂商
  SDN控制器与VxLAN交换机的互通性很差。

- 三是SDN控制器东西向互通方案需要标准化。目前，不同厂商的SDN控制器之间均无法实现
  通信，且各厂商的SDN控制器灾备方案也不尽相同，如有采用群集技术实现方案、有采用
  MP-BGP EVPN实现方案等。

- 四是基于MP-BCP EVPN作为控制平面的VxLAN技术实现虽已部分标准化，但已有解决方案的
  各厂商不仅实现架构不尽相同，而且互通性存在问题。

**** 其他参考
1. [[http://blog.csdn.net/quqi99/article/details/9170109][关于VXLAN与异构云之间的集成]]
2. [[http://blog.csdn.net/jincm13/article/details/8744998][VXLAN：是好是坏？]]
3. [[http://www.sdnlab.com/resource/11943.html][OpenFlow和VxLAN]]
** network namespaces
*** Linux network namespaces
In a network namespace, the scoped ‘identifiers’ are network devices; so a given
network device, such as eth0, exists in a particular namespace.
- *Each network namespace* also *has* its *own routing table*, and in fact this
  is *the main reason* for namespaces *to exist*. A routing table is keyed by
  destination IP address, so network namespaces are what you need if you want
  the same destination IP address to mean different things at different times -
  which is something that OpenStack Networking requires for its *feature* of
  providing *overlapping IP addresses* in different virtual networks.

- Each network namespace also has its own set of iptables (for both IPv4 and
  IPv6).

*** Virtual routing and forwarding (VRF)
Virtual routing and forwarding is an IP technology that allows *multiple
instances of a routing table* to *coexist* on the *same router* at the *same
time*. It is *another name* for the *network namespace* functionality described
above.
** network address translation
*** SNAT (Source Network Address Translation)
在SNAT中，NAT Router修改发送方IP包中的IP地址。SNAT通常用于主机使用私有IP地址与外
网通信。RFC 1918保留了3个私有地址网段：
- 10.0.0.0/18
- 172.16.0.0/12
- 192.168.0.0/16

这些私有IP地址是无法被路由的，也就是说外网的主机无法发送IP包给这些地址。私有地址
广泛地应用在家用和企业环境中。

但是，使用私有IP地址的主机常常需要访问外网。一个常见的例子就是访问www.baidu.com。
如果发往百度的IP包使用私有IP地址为源地址，那么百度服务器将无法返回数据给发出请求
的主机。

SNAT通过修改发送方的源地址为可路由的外网地址来解决这种问题。在实现中，有几种不同
的方案。在OpenStack实现中，通过在发送方和接收方之间增加一个NAT Router来实现。NAT
Router将替换IP包中的源地址为NAT Router使用的公网地址。另外，NAT Router还会修改
TCP或UDP的端口，并维护一个相对应的真实的IP地址和端口。

当NAT路由接收到一个匹配的IP地址和端口时，它将这些转换为私有地址和端口，然后转发
这些IP包。

由于NAT路由不仅修改了IP地址，而且修改了端口，它有时又被称为Port Address
Translation (PAT)，或者NAT overload。

OpenStack使用SNAT来实现虚拟主机中的应用访问外网。
*** DNAT
在Destination Network Address Translation (DNAT)中，NAT Router修改IP包头中的目的
地址。

OpenStack使用DNAT来路由实例到OpenStack metadata service的IP包。实例中的应用通过
使用IP地址169.254.169.254发送HTTP GET请求给Web服务器来访问OpenStack metadata
service。在OpenStack部署中，没有任何实例会使用这个IP地址。作为替代，OpenStack使
用DNAT修改这些IP包的目的IP地址而使用这些包能够到到达OpenStack metadata sevice监
听的网络接口。
*** One-to-One NAT
 在One-to-One NAT中，NAT Router维护私有地址与公有地址的一一映射。OpenStack使用
 One-to-One NAT来实现浮动IP地址。
* Introduction to OpenStack Networking (neutron)
** Overview and components
- API Server
- OpenStack Networking plug-in and agents: It is important to mention that *only
   one plug-in can be used at a time*.
- Messaging queue
*** OpenStack Networking concepts
 两种网络：teneat networks, provider networks
**** Tenant networks
 用户账户创建租户网络用于项目的通信。默认情况下，网络是完全独立的，并且无法与其他项目共享。
 OpenStack Networking支持以下几种网络隔离与复用技术：
 Flat, VLAN, GRE and VXLAN

**** Provider networks
 管理员账户创建供应网络。这些网络与数据中心的真实的物理设置相对应。可用的网络类型
 是FLAT(untagged)和VLAN(802.1Q tagged)。

**** Subnets

**** Ports

**** Routers

**** Security groups

**** Extensions
*** Service and component hierarchy¶
**** Components
 - Server :: provides API, manages database, etc.
 - Plug-ins :: manages agents
 - Agents
   + provides layer 2/3 connectivity to instances
   + handles physical-virtual network transition
   + handles metadata, etc.
 - Layer2 :: Ethernet and Switching
   + Linux Bridge
   + OVS
 - Layer3 :: IP and Routing
   + L3
   + DHCP
 - Miscellaneous :: Metadata

**** Services
 - Routing services
   + VPNaaS :: The Virtual Private Network-as-a-Service is a Neutron extension
   + LbaaS :: The Load-Balancer-as-a-Service is based on the HAProxy software
   + FwaaS ::

*** Configuration
** Service and component hierarchy

* Deployment scenarios[fn:3]									   :noexport:
** Legacy with Open vSwitch
** Legacy with Linux Bridge
** High Availability using Distributed Virtual Routing (DVR)
** High Availability using VRRP (L3HA) with Open vSwitch
** High Availability using VRRP (L3HA) with Linux Bridge
** Provider networks with Open vSwitch
** Provider networks with Linux bridge

* Advanced features through API extensions[fn:2]
** Provider networks
- Provider networks enable cloud administrators to create Networking networks
  that map directly to the physical networks in the data center. This is
  commonly used to give tenants direct access to a public network that can be
  used to reach the Internet.

- It might also be used to integrate with VLANs in the network that already have
  a defined meaning (for example, enable a VM from the marketing department to
  be placed on the same VLAN as bare-metal marketing hosts in the same data
  center).

- The provider extension allows administrators to explicitly manage the
  relationship between Networking virtual networks and underlying physical
  mechanisms such as VLANs and tunnels.

- When this extension is supported, Networking client users with administrative
  privileges see additional provider attributes on all virtual networks and are
  able to specify these attributes in order to create provider networks.

- The provider extension is supported by the Open vSwitch and Linux Bridge
  plug-ins. Configuration of these plug-ins requires familiarity with this
  extension.
** Terminology
A number of terms are used in the provider extension and in the configuration of
plug-ins supporting the provider extension:

Provider extension terminology
| Term             | Description                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|                  | 70                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|  /               |  <                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| virtual network  | A Networking L2 network (identified by a UUID and optional name) whose ports can be attached as vNICs to Compute instances and to various Networking agents. The Open vSwitch and Linux Bridge plug-ins each support several different mechanisms to realize virtual networks.                                                                                                                                                                     |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| physical network | A network connecting virtualization hosts (such as compute nodes) with each other and with other network resources. Each physical network might support multiple virtual networks. The provider extension and the plug-in configurations identify physical networks using simple string names.                                                                                                                                                     |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| tenant network   | A virtual network that a tenant or an administrator creates. The physical details of the network are not exposed to the tenant.                                                                                                                                                                                                                                                                                                                    |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| provider network | A virtual network administratively created to map to a specific network in the data center, typically to enable direct access to non-OpenStack resources on that network. Tenants can be given access to provider networks.                                                                                                                                                                                                                        |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| VLAN network     | A virtual network implemented as packets on a specific physical network containing IEEE 802.1Q headers with a specific VID field value. VLAN networks sharing the same physical network are isolated from each other at L2 and can even have overlapping IP address spaces. Each distinct physical network supporting VLAN networks is treated as a separate VLAN trunk, with a distinct space of VID values. Valid VID values are 1 through 4094. |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| flat network     | A virtual network implemented as packets on a specific physical network containing no IEEE 802.1Q header. Each physical network can realize at most one flat network.                                                                                                                                                                                                                                                                              |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| local network    | A virtual network that allows communication within each host, but not across a network. Local networks are intended mainly for single-node test scenarios, but can have other uses.                                                                                                                                                                                                                                                                |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| GRE network      | A virtual network implemented as network packets encapsulated using GRE. GRE networks are also referred to as tunnels. GRE tunnel packets are routed by the IP routing table for the host, so GRE networks are not associated by Networking with specific physical networks.                                                                                                                                                                       |
|------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| VXLAN network    | VXLAN is a proposed encapsulation protocol for running an overlay network on existing Layer 3 infrastructure. An overlay network is a virtual network that is built on top of existing network Layer 2 and Layer 3 technologies to support elastic compute architectures.                                                                                                                                                                          |

The ML2, Open vSwitch, and Linux Bridge plug-ins support VLAN networks, flat
networks, and local networks. Only the ML2 and Open vSwitch plug-ins currently
support GRE and VXLAN networks, provided that the required features exist in the
hosts Linux kernel, Open vSwitch, and iproute2 packages.

Provider attributes

* SDN、NVF、NV
** Software Define Network
** Network Function Virtualization[fn:7]
在笔者看来，让NFV落地的种种条件直到最近这半年才基本成熟。估计再过半年或者一年市
场上就会出现比较靠谱的NFV解决方案，等明年这个时候可能就会有比较有说服力的案例出
现了。

在这里我就不花笔墨科普NFV了，目前为止见到的最好的解释NFV以及NFV和SDN关系的文章是
sigcomm 2014的OpenNF，特别是它的前两章，建议做SDN和NFV的兄弟们都读一下。这篇文章
的结论之一是没有SDN，NFV是玩儿不转的。但这仅仅是故事的一部分，今天会把我眼中那些
决定NFV落地的关键因素搭个框架出来，细节会在之后的文章陆续展开。

*** 需求：
NFV (Network Function Virtualization，网络功能虚拟化)，如果非要用一句话解释就是：
把在传统网络中只在专门硬件上跑着的功能放到虚拟机里跑，比较典型的例子是把防火墙跑
在虚拟机上。这样做有很多好处：省钱；升级虚拟机比升级硬件方便；根据业务需求弹性部
署；易于管理等等。总之那些主机虚拟化的好处对于NFV同样适用。对NFV最大的需求来自两
类大金主：运营商和云。

运营商会在网络中部署各种各样的middlebox (Network Function的又一种说法，真不明白
人们为什么花精力编造不同的名词来描述同一个东西...)，middlebox种类之繁杂让人一度
目瞪口呆，sigcomm 2012的APLOMB说运营商管理的middlebox的数量比他们管理的路由器加
交换机的总和还多。面对如此庞杂的middlebox，运营商对于NFV的需求也最为旺盛。

对于NFV的另外一个需求大户是云，特别是在多租户大行其道的今天。每一个租户都要在自
*己的网络入口部署防火墙和负载均衡。云服务提供商不可能为每一个租户购买专门的硬件设*
备来完成这些功能。把这些功能跑在虚拟机里几乎是唯一的选择。

对NFV的需求如此强烈，为什么迟迟没有落地呢？因为一些关键的技术问题直到最近才有比
较靠谱的解决方案。

*** 技术：
NFV最大的技术难题是性能。还是拿防火墙来举例：防火墙是有状态的，它要追踪每一个TCP
链接并且根据规则做出判断。人们为防火墙设计专门的芯片就是为了能够线速处理网络流量。
*于是从2000左右开始，硬件防火墙就一直统治着市场。如果把所有这些功能都放到虚拟机，*
放到软件上来做，就意味着中断，数据拷贝，要达到线速非常困难。伴随着基于DPDK，
SR-IOV的一系列方案的不断完善，这个问题得到了比较好的解决。解决思路就是用最少的中
断，寻址和数据拷贝将数据包搬运于网卡和防火墙虚拟机之间。我会写专门的文章比较这两
个技术流派，目前更倾向于认为DPDK会得到更广泛的应用(事实上在大型互联网企业里，基
于的DPDK的应用已经走得很远了)。

以SR-IOV为代表的网卡技术有两个硬伤我还没想清楚怎么破：
1) 没有HA，一个网卡挂了，它所有的virtual function全挂。SR-IOV没有bond的概念。
2) 产品升级困难，唯一的方法就是换网卡，重新部署，重新配置。

NFV面临的第二个难题是根据middlebox的功能和在网络中的位置，高度动态的计算路径，将
流量正确的转发入/出middlebox。这个问题伴随着SDN的落地开花，不少SDN厂家都有了比较
靠谱的解决方案。

*** 部署：
直到上面的两个技术问题得到解决，谈NFV的部署才有意义。NFV的部署其实和虚拟机的编排
没有太本质的区别。最大的不同是需要为middlebox分配专属的硬件资源，比如一个防火墙
的CPU都应该处于一个NUMA上。这些接口在过去半年里也终于被openstack支持了。

讲到这里，大家也就明白为什么笔者会认为NFV离落地不远了：需求旺盛，技术难题已经得
到了比较好的解决，部署方式和现有的编排系统高度相似。NFV真的要来了。

** OpenFlow 南向协议
** iptables
* Footnotes

[fn:7] [[http://www.sdnlab.com/16225.html][NFV要来了]]

[fn:8] http://www.zgazxxw.com/news/jsdt/201512/118038.html

[fn:6] http://www.opencloudblog.com/?p=300

[fn:5] http://www.definethecloud.net/vxlan-deep-dive/

[fn:4] http://book.51cto.com/art/201312/424012.htm

[fn:3] http://docs.openstack.org/liberty/networking-guide/deploy.html

[fn:2] http://docs.openstack.org/admin-guide-cloud/networking_adv-features.html

[fn:1] http://docs.openstack.org/liberty/networking-guide/
