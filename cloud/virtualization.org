* 虚拟 Linux
  虚拟化 就是要将某种形式的东西以另外一种形式呈现出来。对计算机进行虚
  拟化就是要将计算机以多台计算机或一台完全不同的计算机的形式呈现出来。

  虚拟化也可以将多台计算机组合成一台计算机的形式呈现出来。这通常称为服
  务器聚合或网格计算。

* 虚拟化的历史
  虚拟化并不是什么新主题；实际上，它的存在已经超过 40 年了。虚拟化技术
  最早的一些用法包括 IBM® 7044、麻省理工学院（MIT）在 IBM 704 上开发的
  CTSS（Compatible Time Sharing System）以及曼彻斯特大学的 Atlas 项目
  （世界上最早的超级计算机之一），这些都是请求页面调度和监管进程调用的
  先驱。

** 硬件虚拟化
   IBM 早在 20 世纪 60 年代开发 System/360™ Model 67 大型机时就认识到
   了虚拟化的重要性。Model 67 通过 VMM（Virtual Machine Monitor）对所
   有的硬件接口都进行了虚拟化。在早期计算中，操作系统被称为 supervisor。
   能够在其他操作系统上运行的操作系统被称为 hypervisor（这个术语是在
   20 世纪 70 年代出现的）。

   VMM 可以直接在底层硬件上运行，允许运行多个虚拟机（VM）。每个 VM 都
   可以运行一个自己私有操作系统的实例 —— 在早些时候，这称为 CMS（或
   Conversational Monitor System）。之后 VM 继续发展，现在您可以在
   System z9™ 大型机上发现 VM。这提供了很好的向后兼容性，甚至是对
   System/360 产品线的兼容性。

** 处理器虚拟化
   虚拟化早期的另外一种用法（在本例中是对处理器的仿真）是 P-code（或伪
   码）机。P-code 是一种机器语言，运行于虚拟机而不是实际硬件。P-code
   早在 20 世纪 70 年代就已在加州大学圣地亚哥分校（UCSD）Pascal 系统上
   颇有名气了，它将 Pascal 程序编译成 P-code，然后在一个 P-code 虚拟机
   上运行。这就使 P-code 程序具有了高度的可移植性，而且，只要有可用的
   P-code 虚拟机，P-code 程序就可以运行。

   20 世纪 60 年代对 BCPL（Basic Combined Programming Language）的设计
   中也采用了相同的概念，C 语言即由 BCPL 发展而来。在这种用法中，编译
   器会将 BCPL 代码编译成称为 O-code 的中间机器代码。接下来的第二个步
   骤是将 O-code 编译成目标机器的原始语言。现代编译器所使用的这种模型
   为将编译器移植到新目标体系结构上提供了很大的灵活性（通过一种中间语
   言将前端和后端分隔开来）。

** 指令集虚拟化
   虚拟化最新的发展称为指令集虚拟化，或者二进制转换。在这种模型中，虚
   拟指令集被转换成底层硬件的物理指令集，这个过程通常都是动态的。当代
   码执行时，就会对代码的某个段进行转换。如果出现分支情况，就会导入新
   代码集并进行转换。这使它与缓存操作非常类似，后者是将指令块从内存移
   动到本地快速缓存中执行。

   这种模型最近在 Transmeta 设计的 Crusoe 中央处理单元（CPU）中得到了
   使用。二进制转换由 Code Morphing 的专利技术实现。类似的一个例子是完
   全虚拟化解决方案通过运行时代码扫描来查找和重定向特权指令（用来解决
   特定处理器指令集的一些问题）。

* 虚拟化的类型
  实现虚拟化的方法不止一种。实际上，有几种方法都可以通过不同层次的抽象
  来实现相同的结果。本节将介绍 Linux 中常用的 3 种虚拟化方法，以及它们
  相应的优缺点。业界有时会使用不同的术语来描述相同的虚拟化方法。本文中
  使用的是最常用的术语，同时给出了其他术语以供参考。

** 硬件仿真
   毫无疑问，最复杂的虚拟化实现技术就是硬件仿真。在这种方法中，可以在
   宿主系统上创建一个硬件 VM 来仿真所想要的硬件

   使用硬件仿真的主要问题是速度会非常慢。由于每条指令都必须在底层硬件
   上进行仿真，因此速度减慢 100 倍的情况也并不稀奇。若要实现高度保真的
   仿真，包括周期精度、所仿真的 CPU 管道以及缓存行为，实际速度差距甚至
   可能会达到 1000 倍之多。

   硬件仿真也有自己的优点 。例如，使用硬件仿真，您可以在一个 ARM 处理器
   主机上运行为 PowerPC® 设计的操作系统，而不需要任何修改。您甚至可以
   运行多个虚拟机，每个虚拟器仿真一个不同的处理器。

** 完全虚拟化
   完全虚拟化（full virtualization），也称为原始虚拟化，是另外一种虚拟
   化方法。这种模型使用一个虚拟机，它在客户操作系统和原始硬件之间进行
   协调（参见图 2）。“协调”在这里是一个关键，因为 VMM 在客户操作系统
   和裸硬件之间提供协调。特定受保护的指令必须被捕获下来并在 hypervisor
   中进行处理，因为这些底层硬件并不由操作系统所拥有，而是由操作系统通
   过 hypervisor 共享。

   虽然完全虚拟化的速度比硬件仿真的速度要快，但是其性能要低于裸硬件，
   因为中间经过了 hypervisor 的协调过程。完全虚拟化的最大优点是操作系
   统无需任何修改就可以直接运行。惟一的限制是操作系统必须要支持底层硬
   件（例如 PowerPC）。

** 超虚拟化
   超虚拟化（paravirtualization）是另外一种流行的虚拟化技术，它与完全
   虚拟化有一些类似。

   这种方法使用了一个 hypervisor 来实现对底层硬件的共享访问，还将与虚
   拟化有关的代码集成到了操作系统本身中（参见图 3）。这种方法不再需要
   重新编译或捕获特权指令，因为操作系统本身在虚拟化进程中会相互紧密协
   作。

   超虚拟化技术需要为 hypervisor 修改客户操作系统，这是它的一个缺点。
   但是超虚拟化提供了与未经虚拟化的系统相接近的性能。与完全虚拟化类似，
   超虚拟化技术可以同时支持多个不同的操作系统。

** 操作系统级的虚拟化
   这种技术在操作系统本身之上实现服务器的虚拟化。这种方法支持单个操作
   系统，并可以将独立的服务器相互简单地隔离开来（参见图 4）。

   操作系统级的虚拟化要求对操作系统的内核进行一些修改，但是其优点是可
   以获得原始性能。

* 为什么虚拟化如此重要
  从商业角度来看，使用虚拟化技术有很多原因。大部分原因都可以归结于服务
  器的巩固（server consolidation）。简单来说，如果您可以对一个服务器上
  多个未经充分利用的系统进行虚拟化，由于服务器的数量少了，显然可以节省
  大量电力、空间、制冷和管理成本。由于很难确定服务器的利用情况，虚拟化
  技术支持称为动态迁移的技术。动态迁移（Live migration）允许操作系统及
  其应用程序迁移到新的服务器上，从而实现负载在可用硬件上的均衡

  虚拟化技术对于开发人员来说也非常重要。Linux 内核占据了一个单一的地址
  空间，这意味着内核或任何驱动程序的故障都会导致整个操作系统的崩溃。虚
  拟化技术意味着您可以运行多个操作系统，如果其中一个系统由于某个 bug
  而崩溃了，那么 hypervisor 和其他操作系统都依然可以继续运行。这可以使
  内核的调试非常类似于用户空间应用程序的调试。

* 与 Linux 有关的虚拟化项目
  | 项目                 | 类型             | 许可证   |
  | Bochs                | 仿真             | LGPL     |
  | QEMU                 | 仿真             | LGPL/GPL |
  | VMware               | 完全虚拟化       | 私有     |
  | z/VM                 | 完全虚拟化       | 私有     |
  | Xen                  | 超虚拟化         | GPL      |
  | UML(User-mode Linux) | 超虚拟化         | GPL      |
  | Linux-VServer        | 操作系统级虚拟化 | GPL      |
  | OpenVZ               | 操作系统级虚拟化 | GPL      |
  | Linux KVM            | 完全虚拟化       | GPL      |

* Linux KVM（内核虚拟机） 
  Linux 传出的最新消息是将 KVM 合并到 Linux 内核中（2.6.20）。KVM 是一
  种完全虚拟化解决方案，它有一个方面非常独特：它将 Linux 内核转换为一
  个使用内核模块的 hypervisor。这个模块允许使用其他客户操作系统，然后
  在宿主 Linux 内核的用户空间中运行（参见图 7）。内核中的 KVM 通过
  /dev/kvm 字符设备来公开虚拟化后的硬件。客户操作系统使用为 PC 硬件仿
  真修改过的 QEMU 进程与 KVM 模块接口。

  KVM 模块向内核中引入了一个新的执行模块。普通内核支持内核 模式和用户
  模式，而 KVM 则引入了一种客户 模式。客户模式用来执行所有非 I/O 客户
  代码，而普通用户模式支持客户 I/O。

* 网络虚拟化
  在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其
  他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交
  换机上，如下图所示。

  这种架构下，为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，
  这样会存在两个问题，1）是某些应用大部分情况可能处于空闲状态，2）是当
  应用增多的时候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这
  种架构都会对物理资源造成极大的浪费。

  为了解决这个问题，可以借助虚拟化技术对一台物理资源进行抽象，将一张物
  理网卡虚拟成多张虚拟网卡（vNIC），通过虚拟机来隔离不同的应用。

  这样对于上面的问题 1），可以利用虚拟化层 Hypervisor 的调度技术，将资
  源从空闲的应用上调度到繁忙的应用上，达到资源的合理利用；针对问题 2），
  可以根据物理设备的资源使用情况进行横向扩容，除非设备资源已经用尽，否
  则没有必要新增设备。其中虚拟机与虚拟机之间的通信，由虚拟交换机完成，
  虚拟网卡和虚拟交换机之间的链路也是虚拟的链路，整个主机内部构成了一个
  虚拟的网络，如果虚拟机之间涉及到三层的网络包转发，则又由另外一个角
  色——虚拟路由器来完成。

  总结下来，网络虚拟化主要解决的是虚拟机构成的网络通信问题，完成的是各
  种网络设备的虚拟化，如网卡、交换设备、路由设备等。

** Linux 下网络设备虚拟化的几种形式
   Linux 本身由于虚拟化技术的演进，也集成了一些虚拟网络设备的解决方案，
   主要有以下几种：
   1. TAP/TUN/VETH 

      TAP/TUN 是 Linux 内核实现的一对虚拟网络设备，TAP 工作在二层，TUN
      工作在三层。Linux 内核通过 TAP/TUN 设备向绑定该设备的用户空间程
      序发送数据，反之，用户空间程序也可以像操作物理网络设备那样，向
      TAP/TUN 设备发送数据。
      
      基于 TAP 驱动，即可实现虚拟机 vNIC 的功能，虚拟机的每个 vNIC 都
      与一个 TAP 设备相连，vNIC 之于 TAP 就如同 NIC 之于 eth。当一个
      TAP 设备被创建时，在 Linux 设备文件目录下会生成一个对应的字符设
      备文件，用户程序可以像打开一个普通文件一样对这个文件进行读写。

      TUN 则属于网络中三层的概念，数据收发过程和 TAP 是类似的，只不过
      它要指定一段 IPv4 地址或 IPv6 地址，并描述其相关的配置信息，其数
      据处理过程也是类似于普通物理网卡收到三层 IP 报文数据。

      VETH 设备总是成对出现，一端连着内核协议栈，另一端连着另一个设备，
      一个设备收到内核发送的数据后，会发送到另一个设备上去，这种设备通
      常用于容器中两个 namespace 之间的通信。
      
   2. Bridge
      
      Bridge 也是 Linux 内核实现的一个工作在二层的虚拟网络设备，但不同
      于 TAP/TUN 这种单端口的设备，Bridge 实现为多端口，本质上是一个虚
      拟交换机，具备和物理交换机类似的功能。

      Bridge 可以绑定其他 Linux 网络设备作为从设备，并将这些从设备虚拟
      化为端口，当一个从设备被绑定到 Bridge 上时，就相当于真实网络中的
      交换机端口上插入了一根连有终端的网线。

      因为 Bridge 工作在二层，所以绑定到它上面的从设备 eth0、tap0、
      tap1 均不需要设 IP，但是需要为 br0 设置 IP，因为对于上层路由器来
      说，这些设备位于同一个子网，需要一个统一的 IP 将其加入路由表中。

      这里有人可能会有疑问，Bridge 不是工作在二层吗，为什么会有 IP 的
      说法？其实 Bridge 虽然工作在二层，但它只是 Linux 网络设备抽象的
      一种，能设 IP 也不足为奇。

      对于实际设备 eth0 来说，本来它是有自己的 IP 的，但是绑定到 br0
      之后，其 IP 就生效了，就和 br0 共享一个 IP 网段了，在设路由表的
      时候，就需要将 br0 设为目标网段的地址。

   3. veth
      
      Linux container 中用到一个叫做veth的东西，这是一种新的设备，专门
      为 container 所建。veth 从名字上来看是 Virtual ETHernet 的缩写，
      它的作用很简单，就是要把从一个 network namespace 发出的数据包转
      发到另一个 namespace。veth 设备是成对的，一个是 container 之中，
      另一个在 container 之外，即在真实机器上能看到的。

      VETH设备总是成对出现，送到一端请求发送的数据总是从另一端以请求接
      受的形式出现。创建并配置正确后，向其一端输入数据，VETH会改变数据
      的方向并将其送入内核网络子系统，完成数据的注入，而在另一端则能读
      到此数据。（Namespace，其中往veth设备上任意一端上RX到的数据，都
      会在另一端上以TX的方式发送出去）veth工作在L2数据链路层，
      veth-pair设备在转发数据包过程中并不串改数据包内容。

* 参考
  - https://www.ibm.com/developerworks/cn/linux/l-linuxvirt/
  - https://www.cnblogs.com/woshiweige/p/4532207.html
  - TUN/TAP: https://blog.kghost.info/2013/03/27/linux-network-tun/
  - https://www.fir3net.com/Networking/Terms-and-Concepts/virtual-networking-devices-tun-tap-and-veth-pairs-explained.html
